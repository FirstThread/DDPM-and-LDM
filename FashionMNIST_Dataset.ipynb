{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00beb01c",
   "metadata": {},
   "source": [
    "#### Getting Started with FashionMNIST\n",
    "\n",
    "This notebook shows how to load the FashionMNIST dataset in PyTorch and illustrates what kind of samples it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9335503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import ConcatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d0a6844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The combined dataset contains 70000 samples.\n",
      "It contains objects from the following 10 classes:\n",
      "['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'] \n",
      "\n",
      "The shape of an image tensor is: torch.Size([1, 28, 28])\n",
      "The shape of a class label is: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# create an instance of the training split of the FashionMNIST dataset\n",
    "dataset_train = FashionMNIST(root='/data/FashionMNIST', train=True, download=True, \n",
    "                             transform=ToTensor(), target_transform=lambda x: torch.Tensor([x]).int())\n",
    "# now create an instance of the test split\n",
    "dataset_valid = FashionMNIST(root='/data/FashionMNIST', train=False, download=True, \n",
    "                             transform=ToTensor(), target_transform=lambda x: torch.Tensor([x]).int())\n",
    "# for our purposes we can combine the 60k training and 10k testing samples\n",
    "dataset = ConcatDataset([dataset_train, dataset_valid])\n",
    "# display some information on the dataset\n",
    "print(f'The combined dataset contains {len(dataset)} samples.')\n",
    "print(f'It contains objects from the following {len(dataset_train.classes)} classes:')\n",
    "print(dataset_train.classes, '\\n')\n",
    "# select a random sample index and load the corresponding data sample\n",
    "sample_idx = random.randrange(len(dataset))\n",
    "# \n",
    "img_tensor, class_label = dataset.__getitem__(sample_idx)\n",
    "print('The shape of an image tensor is:', img_tensor.shape)\n",
    "print('The shape of a class label is:', class_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "166dbb4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image shows an instance of Dress.\n"
     ]
    }
   ],
   "source": [
    "# we can also visualize and describe a sample\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(img_tensor.squeeze(), cmap='Greys_r')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print('The image shows an instance of ' + dataset_train.classes[class_label.item()] + '.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
